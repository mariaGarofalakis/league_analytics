{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/29 13:46:05 WARN Utils: Your hostname, pc, resolves to a loopback address: 127.0.1.1; using 192.168.0.5 instead (on interface wlxc04a00119951)\n",
      "25/11/29 13:46:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/29 13:46:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/29 13:46:07 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:469)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:99)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1056)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1054)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.org$apache$spark$sql$classic$SparkSession$$applyExtensions(SparkSession.scala:1054)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.applyAndLoadExtensions(SparkSession.scala:1038)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.<init>(SparkSession.scala:116)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test Bronze Ingest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, expr, count\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from pyspark.sql.utils import AnalysisException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_df = (\n",
    "        spark.read.option(\"header\", True)\n",
    "        .csv(\"../data/schedule.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- game_id: string (nullable = true)\n",
      " |-- round: string (nullable = true)\n",
      " |-- home_team: string (nullable = true)\n",
      " |-- away_team: string (nullable = true)\n",
      " |-- game_start_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schedule_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+---------------+--------------------+\n",
      "|             game_id|round|        home_team|      away_team|     game_start_time|\n",
      "+--------------------+-----+-----------------+---------------+--------------------+\n",
      "|9a7624be518dffb4c...|    1|         West Ham| Crystal Palace|2025-08-22T15:00:00Z|\n",
      "|55970f64637d0f56d...|    1|           Wolves| Manchester Utd|2025-08-22T20:00:00Z|\n",
      "|15feb050f27b2c46e...|    1|      Aston Villa|      Brentford|2025-08-23T18:00:00Z|\n",
      "|f2bc560c91217f7e3...|    1|          Everton|       Brighton|2025-08-23T19:00:00Z|\n",
      "|fcc94bed9cfb4412d...|    1|Tottenham Hotspur|        Burnley|2025-08-23T20:00:00Z|\n",
      "|de44a47a4dfb2803d...|    1|           Fulham|     Nottingham|2025-08-23T20:00:00Z|\n",
      "|73a4567666ebd9fcc...|    1|          Arsenal|        Chelsea|2025-08-24T15:00:00Z|\n",
      "|f21b2961c297b2c60...|    1|        Liverpool|   Leeds United|2025-08-24T19:00:00Z|\n",
      "|7b231677734908a6a...|    1|      Bournemouth|Manchester City|2025-08-24T20:00:00Z|\n",
      "|07b3a6ff878548296...|    1|        Newcastle|     Sunderland|2025-08-25T18:00:00Z|\n",
      "|5274552fdd6188eeb...|    2|      Aston Villa|     Sunderland|2025-08-29T15:00:00Z|\n",
      "|ba62488f3b28d8fc1...|    2|     Leeds United|        Arsenal|2025-08-29T19:00:00Z|\n",
      "|f04f165c57b424f86...|    2|         West Ham|       Brighton|2025-08-29T20:00:00Z|\n",
      "|f47b46d09f6959bee...|    2|          Everton|     Nottingham|2025-08-29T20:00:00Z|\n",
      "|fdd011e399610f2c0...|    2|           Wolves|Manchester City|2025-08-30T15:00:00Z|\n",
      "|0fb3dcae1fe83c76d...|    2|        Liverpool|      Brentford|2025-08-31T15:00:00Z|\n",
      "|9c9f1674b2d1f2400...|    2|        Newcastle|        Burnley|2025-08-31T18:00:00Z|\n",
      "|09be137260fd71d52...|    2|           Fulham| Manchester Utd|2025-08-31T18:00:00Z|\n",
      "|2afe19c18be105488...|    2|      Bournemouth|        Chelsea|2025-09-01T18:00:00Z|\n",
      "|655d781effd7578ce...|    2|Tottenham Hotspur| Crystal Palace|2025-09-01T20:00:00Z|\n",
      "+--------------------+-----+-----------------+---------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "schedule_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many different teams we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(schedule_df.select(\"home_team\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(schedule_df.select(\"away_team\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since teams are 20 and we have 38 rounds we need to have 20x38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if game_id has unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_unique(df: DataFrame, column: str) -> tuple[bool, str]:\n",
    "    total = df.count()\n",
    "    distinct = df.select(column).distinct().count()\n",
    "    if total != distinct:\n",
    "        return False, f\"Column '{column}' has {total - distinct} duplicate values\"\n",
    "    return True, f\"Column '{column}' is unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, \"Column 'game_id' is unique\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_unique(schedule_df, \"game_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if round containes only numbers (integers) and if it is between 1 and 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_values_between(\n",
    "            df: DataFrame, column: str, min_val: int, max_val: int\n",
    "        ) -> tuple[bool, str]:\n",
    "\n",
    "            # Use try_cast to avoid Spark exceptions\n",
    "            cleaned = df.withColumn(column, expr(f\"try_cast({column} as int)\"))\n",
    "\n",
    "            invalid = cleaned.filter(\n",
    "                col(column).isNull() |\n",
    "                (col(column) < min_val) |\n",
    "                (col(column) > max_val)\n",
    "            ).count()\n",
    "\n",
    "            if invalid > 0:\n",
    "                return False, (\n",
    "                    f\"Column '{column}' has {invalid} invalid values \"\n",
    "                    f\"(non-numeric or outside range [{min_val}, {max_val}])\"\n",
    "                )\n",
    "\n",
    "            return True, f\"Column '{column}' values are valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, \"Column 'round' values are valid\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_values_between(schedule_df,\"round\", 1, 38 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that home_team and away_team are not the same in one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_columns(df: DataFrame, col1: str, col2: str) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Expect that two columns are completely different in every row.\n",
    "        Returns (False, message) if any row has col1 == col2 (or both null).\n",
    "        \"\"\"\n",
    "        if col1 not in df.columns or col2 not in df.columns:\n",
    "            return False, f\"Missing one or both columns: '{col1}', '{col2}'\"\n",
    "\n",
    "        same_rows = df.filter(\n",
    "            (col(col1) == col(col2)) | (col(col1).isNull() & col(col2).isNull())\n",
    "        ).count()\n",
    "\n",
    "        if same_rows > 0:\n",
    "            return False, f\"Found {same_rows} rows where '{col1}' == '{col2}'\"\n",
    "        return True, f\"All rows have different values for '{col1}' and '{col2}'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, \"All rows have different values for 'home_team' and 'away_team'\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_columns(schedule_df, \"home_team\", \"away_team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check in home_team and away_team that teams appear once in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_unique_per_round(df: DataFrame, column: str) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Expect that each team appears only once per round.\n",
    "        Returns False if a team appears more than once as home_team or away_team in a round.\n",
    "        \"\"\"\n",
    "        \n",
    "        duplicates = (\n",
    "            df.groupBy(\"round\", column)\n",
    "            .agg(count(\"*\").alias(\"count\"))\n",
    "            .filter(col(\"count\") > 1)\n",
    "        )\n",
    "\n",
    "        dupe_count = duplicates.count()\n",
    "\n",
    "        if dupe_count > 0:\n",
    "            return False, f\"Found {dupe_count} duplicate {column}(s) within the same round\"\n",
    "        return True, f\"Each {column} appears only once per round\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Each home_team appears only once per round')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_unique_per_round(schedule_df,\"home_team\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Each away_team appears only once per round')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_unique_per_round(schedule_df,\"away_team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focous on game_start_time \n",
    "1. all the values should be a valid date time.\n",
    "2. values should be between 25/8/2025 - 18/5/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_datetime_in_range(\n",
    "        df: DataFrame,\n",
    "        column: str,\n",
    "        start_date: datetime,\n",
    "        end_date: datetime\n",
    "    ) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates that a datetime column contains:\n",
    "        1. Only valid parsable timestamps\n",
    "        2. All values between start_date and end_date (inclusive)\n",
    "\n",
    "        Automatically parses strings to timestamp if needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Try parsing column to timestamp if needed\n",
    "            df_ts = df.withColumn(\"parsed_ts\", to_timestamp(col(column)))\n",
    "\n",
    "            # Check unparseable rows\n",
    "            invalid_ts_count = df_ts.filter(col(\"parsed_ts\").isNull()).count()\n",
    "            if invalid_ts_count > 0:\n",
    "                return False, f\"Column '{column}' has {invalid_ts_count} unparseable datetime values\"\n",
    "\n",
    "            # Now check date range (inclusive)\n",
    "            df_range = df_ts.filter(\n",
    "                (col(\"parsed_ts\") < start_date) | (col(\"parsed_ts\") > end_date)\n",
    "            )\n",
    "            out_of_range = df_range.count()\n",
    "\n",
    "            if out_of_range > 0:\n",
    "                return False, (\n",
    "                    f\"Column '{column}' has {out_of_range} values outside range \"\n",
    "                    f\"The values are {df_range.show()}\"\n",
    "                    f\"[{start_date.date()} to {end_date.date()}]\"\n",
    "                )\n",
    "\n",
    "            return True, f\"All values in column '{column}' are valid datetimes within range.\"\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            return False, f\"Failed to analyze column '{column}': {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Unexpected error validating column '{column}': {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " \"All values in column 'game_start_time' are valid datetimes within range.\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_datetime_in_range(schedule_df,\"game_start_time\", datetime(2025, 8, 22), datetime(2026,5,18))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
