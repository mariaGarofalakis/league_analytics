{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test Bronze Ingest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, expr, count\n",
    "from pyspark.sql.functions import col, when, sum as spark_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = (\n",
    "        spark.read.option(\"header\", True)\n",
    "        .csv(\"../data/scores.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- game_id: string (nullable = true)\n",
      " |-- home_goals: string (nullable = true)\n",
      " |-- away_goals: string (nullable = true)\n",
      " |-- ingestion_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------------------+\n",
      "|             game_id|home_goals|away_goals|      ingestion_time|\n",
      "+--------------------+----------+----------+--------------------+\n",
      "|9a7624be518dffb4c...|       2.0|       1.0|2025-08-22T17:47:56Z|\n",
      "|55970f64637d0f56d...|       2.0|       3.0|2025-08-22T22:29:28Z|\n",
      "|750ba4567999ebd97...|       1.0|       1.0|2025-08-23T20:27:10Z|\n",
      "|15feb050f27b2c46e...|       4.0|       0.0|2025-08-23T20:37:18Z|\n",
      "|f2bc560c91217f7e3...|       2.0|       2.0|2025-08-23T21:03:21Z|\n",
      "|de44a47a4dfb2803d...|       1.0|       1.0|2025-08-23T22:30:55Z|\n",
      "|fcc94bed9cfb4412d...|       3.0|       0.0|2025-08-23T22:31:50Z|\n",
      "|73a4567666ebd9fcc...|       1.0|       2.0|2025-08-24T17:29:04Z|\n",
      "|73a4567666ebd9fcc...|       1.0|       2.0|2025-08-24T17:29:04Z|\n",
      "|f21b2961c297b2c60...|       5.0|       1.0|2025-08-24T21:14:52Z|\n",
      "|7b231677734908a6a...|       0.0|       5.0|2025-08-24T22:51:04Z|\n",
      "|07b3a6ff878548296...|       4.0|       0.0|2025-08-25T20:48:08Z|\n",
      "|5274552fdd6188eeb...|       1.0|       2.0|2025-08-29T17:06:34Z|\n",
      "|ba62488f3b28d8fc1...|       1.0|       1.0|2025-08-29T21:23:46Z|\n",
      "|f04f165c57b424f86...|       1.0|       1.0|2025-08-29T22:16:29Z|\n",
      "|f47b46d09f6959bee...|       0.0|       0.0|2025-08-29T22:36:01Z|\n",
      "|fdd011e399610f2c0...|       1.0|       3.0|2025-08-30T17:34:01Z|\n",
      "|0fb3dcae1fe83c76d...|       4.0|       0.0|2025-08-31T17:50:39Z|\n",
      "|9c9f1674b2d1f2400...|       3.0|       0.0|2025-08-31T20:00:25Z|\n",
      "|09be137260fd71d52...|       0.0|       2.0|2025-08-31T20:57:38Z|\n",
      "+--------------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "scores_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 extra values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many different teams we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since teams are 20 and we have 38 rounds we need to have 20x38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if game_id has unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_unique(df: DataFrame, column: str) -> tuple[bool, str]:\n",
    "    total = df.count()\n",
    "    distinct = df.select(column).distinct().count()\n",
    "    if total != distinct:\n",
    "        return False, f\"Column '{column}' has {total - distinct} duplicate values\"\n",
    "    return True, f\"Column '{column}' is unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, \"Column 'game_id' has 5 duplicate values\")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_unique(scores_df, \"game_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify the dublicates in game_id and check if the rows are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------+----------+--------------------+\n",
      "|game_id                         |home_goals|away_goals|ingestion_time      |\n",
      "+--------------------------------+----------+----------+--------------------+\n",
      "|73a4567666ebd9fcc18424d19ef4d4b7|1.0       |2.0       |2025-08-24T17:29:04Z|\n",
      "|73a4567666ebd9fcc18424d19ef4d4b7|1.0       |2.0       |2025-08-24T17:29:04Z|\n",
      "|2a656651aa83c4f9d7c9a2ef926e3d4d|3.0       |2.0       |2025-11-10T20:50:55Z|\n",
      "|2a656651aa83c4f9d7c9a2ef926e3d4d|NULL      |NULL      |2025-11-10T20:50:55Z|\n",
      "|c0bd9b33b58e37d501d3a19c5aa6a86f|2.0       |1.0       |2025-11-16T17:44:33Z|\n",
      "|c0bd9b33b58e37d501d3a19c5aa6a86f|2.0       |1.0       |2025-11-16T17:44:33Z|\n",
      "|9c0a9f78fc51bbc8b88dd7d3422c65a7|2.0       |0.0       |2026-02-14T21:06:21Z|\n",
      "|9c0a9f78fc51bbc8b88dd7d3422c65a7|2.0       |0.0       |2026-02-14T21:06:21Z|\n",
      "|18b68ec5b5ca681ead406efc54d73ff4|0.0       |2.0       |2026-04-11T20:42:57Z|\n",
      "|18b68ec5b5ca681ead406efc54d73ff4|0.0       |2.0       |2026-04-11T20:42:57Z|\n",
      "+--------------------------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicate_ids = (\n",
    "    scores_df.groupBy(\"game_id\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .filter(col(\"count\") > 1)\n",
    "    .select(\"game_id\")\n",
    ")\n",
    "\n",
    "# Step 2: Join back to original DataFrame to show full duplicate rows\n",
    "dupe_rows = scores_df.join(duplicate_ids, on=\"game_id\", how=\"inner\")\n",
    "\n",
    "# Step 3: Show result\n",
    "dupe_rows.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dublicate rows in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = scores_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now just drop rows with null values (there should be a logic to fix nulls in order not to loose info but it is outside of this scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = scores_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have one extra value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if round containes only numbers (integers) and if it is between 1 and 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def column_values_between(\n",
    "    df: DataFrame,\n",
    "    column: str,\n",
    "    min_val: Optional[float] = None,\n",
    "    max_val: Optional[float] = None\n",
    ") -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate that values in the column can be cast to numeric\n",
    "    and are within min/max if provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cast to double using Spark SQL expression\n",
    "        df_casted = df.withColumn(column, expr(f\"CAST({column} AS DOUBLE)\"))\n",
    "\n",
    "        condition = col(column).isNull()\n",
    "\n",
    "        if min_val is not None and max_val is not None:\n",
    "            condition |= (col(column) < min_val) | (col(column) > max_val)\n",
    "            range_msg = f\"outside range [{min_val}, {max_val}]\"\n",
    "        elif min_val is not None:\n",
    "            condition |= col(column) < min_val\n",
    "            range_msg = f\"less than {min_val}\"\n",
    "        elif max_val is not None:\n",
    "            condition |= col(column) > max_val\n",
    "            range_msg = f\"greater than {max_val}\"\n",
    "        else:\n",
    "            range_msg = \"not numeric\"\n",
    "\n",
    "        invalid_count = df_casted.filter(condition).count()\n",
    "\n",
    "        if invalid_count > 0:\n",
    "            return False, f\"Column '{column}' has {invalid_count} invalid values ({range_msg}) where {df_casted.filter(condition).show()}\"\n",
    "        return True, f\"Column '{column}' passed numeric and range validation\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Validation error on column '{column}': {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, \"Column 'home_goals' passed numeric and range validation\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_values_between(scores_df,\"home_goals\", 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, \"Column 'away_goals' passed numeric and range validation\")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_values_between(scores_df,\"away_goals\", 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, \"Column 'away_goals' has 1 invalid values (less than 0)\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_values_between(scores_df,\"away_goals\", 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that ingestion_time is between same times as game_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import to_timestamp, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_datetime_in_range(\n",
    "        df: DataFrame,\n",
    "        column: str,\n",
    "        start_date: datetime,\n",
    "        end_date: datetime\n",
    "    ) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates that a datetime column contains:\n",
    "        1. Only valid parsable timestamps\n",
    "        2. All values between start_date and end_date (inclusive)\n",
    "\n",
    "        Automatically parses strings to timestamp if needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Try parsing column to timestamp if needed\n",
    "            df_ts = df.withColumn(\"parsed_ts\", to_timestamp(col(column)))\n",
    "\n",
    "            # Check unparseable rows\n",
    "            invalid_ts_count = df_ts.filter(col(\"parsed_ts\").isNull()).count()\n",
    "            if invalid_ts_count > 0:\n",
    "                return False, f\"Column '{column}' has {invalid_ts_count} unparseable datetime values\"\n",
    "\n",
    "            # Now check date range (inclusive)\n",
    "            df_range = df_ts.filter(\n",
    "                (col(\"parsed_ts\") < start_date) | (col(\"parsed_ts\") > end_date)\n",
    "            )\n",
    "            out_of_range = df_range.count()\n",
    "\n",
    "            if out_of_range > 0:\n",
    "                return False, (\n",
    "                    f\"Column '{column}' has {out_of_range} values outside range \"\n",
    "                    f\"The values are {df_range.show()}\"\n",
    "                    f\"[{start_date.date()} to {end_date.date()}]\"\n",
    "                )\n",
    "\n",
    "            return True, f\"All values in column '{column}' are valid datetimes within range.\"\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            return False, f\"Failed to analyze column '{column}': {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Unexpected error validating column '{column}': {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " \"All values in column 'ingestion_time' are valid datetimes within range.\")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_datetime_in_range(scores_df,\"ingestion_time\", datetime(2025, 8, 22), datetime(2026,5,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the basic checks are done now we need to implement some buisness logic check to see why we have this extra value in scores\n",
    "1. We need to check if the scores.csv have the same game_id as the schedule_id (I believe there should be one extra value in score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_df = (\n",
    "        spark.read.option(\"header\", True)\n",
    "        .csv(\"../data/schedule.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_ids_match(\n",
    "        df1: DataFrame,\n",
    "        df2: DataFrame,\n",
    "        id_column: str = \"game_id\"\n",
    "    ) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Check whether the given column (default: 'game_id') has exactly the same values\n",
    "        in both DataFrames, and print differences as lists.\n",
    "        \"\"\"\n",
    "        ids1 = df1.select(id_column).distinct()\n",
    "        ids2 = df2.select(id_column).distinct()\n",
    "\n",
    "        # Rows only in df1 and only in df2\n",
    "        only_in_2 = ids2.subtract(ids1)\n",
    "        only_in_1 = ids1.subtract(ids2)\n",
    "\n",
    "        missing_in_2 = only_in_2.count()\n",
    "        missing_in_1 = only_in_1.count()\n",
    "\n",
    "        if missing_in_1 == 0 and missing_in_2 == 0:\n",
    "            return True, f\"✅ Both DataFrames contain the same set of '{id_column}' values.\"\n",
    "\n",
    "        # Convert results to Python lists for printing\n",
    "        list_2 = [row[id_column] for row in only_in_2.collect()]\n",
    "        list_1 = [row[id_column] for row in only_in_1.collect()]\n",
    "\n",
    "        return False, (\n",
    "            f\"❌ Mismatch in '{id_column}':\"\n",
    "            f\"- {missing_in_2} missing in df1: {list_2}\"\n",
    "            f\"- {missing_in_1} missing in df2: {list_1}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " \"❌ Mismatch in 'game_id': \\\\n             - 1 missing in df1: ['750ba4567999ebd97nv18424d81bf412lz'] \\\\n            - 0 missing in df2: [] \\\\n\")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_ids_match(schedule_df,scores_df,\"game_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------------------+\n",
      "|             game_id|home_goals|away_goals|      ingestion_time|\n",
      "+--------------------+----------+----------+--------------------+\n",
      "|750ba4567999ebd97...|       1.0|       1.0|2025-08-23T20:27:10Z|\n",
      "+--------------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.filter(col(\"game_id\")==\"750ba4567999ebd97nv18424d81bf412lz\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely drop this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
